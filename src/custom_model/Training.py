import os
import sys
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# data.py è·¨è³‡æ–™å¤¾ï¼Œæ‰€ä»¥éœ€è¦é¡å¤–å‹•ä½œä¾†è¼”åŠ© import
# 1. å–å¾—ç›®å‰æª”æ¡ˆçš„ (Training.py) æ‰€åœ¨ç›®éŒ„
current_dir = os.path.dirname(os.path.abspath(__file__))

# 2. å–å¾—ä¸Šä¸€å±¤ç›®éŒ„ (å°ˆæ¡ˆçš„æ ¹ç›®éŒ„)
parent_dir = os.path.dirname(current_dir)
Project_Root = os.path.dirname(parent_dir)

# 3. å°‡æ ¹ç›®éŒ„åŠ å…¥ç³»çµ±æœå°‹è·¯å¾‘
sys.path.append(parent_dir)

# 4. é–‹å§‹ import
from src.data import MyDataset

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from Convolution_Class import  CNN
import torchvision
import  matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd

# print(torch.cuda.is_available())

# ---------------------------------
# 1. è¨­å®šåƒæ•¸èˆ‡è£ç½®
# ---------------------------------
BATCH_SIZE = 16 # æ ¹æ“šé¡¯å¡è¨˜æ†¶é«”èª¿æ•´ (16 æˆ– 32)
Learning_Rate = 0.001 # Adam çš„æ¨™æº–å­¸ç¿’ç‡
Num_Epoch = 20


# ----------------------------------
# 2. æº–å‚™è³‡æ–™
# ----------------------------------

csv_path = os.path.join(Project_Root, "_gkcN1hzqm1RFcsvpk5Xmg", 'stitched_pano_final.csv')
img_path = os.path.join(Project_Root, "_gkcN1hzqm1RFcsvpk5Xmg")
df = pd.read_csv(csv_path)

'''
# -------------------------------------------------------
# ğŸ”¥ éæ“¬åˆæ¸¬è©¦æ¨¡å¼ (Overfit Test Mode)
# ç›®çš„ï¼šæª¢æŸ¥ç¨‹å¼é‚è¼¯æœ‰æ²’æœ‰å¯«éŒ¯ï¼Œç¢ºèªæ¨¡å‹èƒ½ä¸èƒ½ã€Œæ­»è¨˜ç¡¬èƒŒã€
# ---------------------------------------------------------

# 1. å…ˆå»ºç«‹å®Œæ•´çš„ Dataset (è·ŸåŸæœ¬ä¸€æ¨£)
full_dataset = MyDataset(
    csv_data=pd.read_csv(csv_path),
    img_dir=img_path,
    is_train=True  # å…ˆé–‹è‘—å¢å¼·æ²’é—œä¿‚ï¼Œå¼·çš„æ¨¡å‹æ‡‰è©²ä¹Ÿè¦èƒ½èƒŒèµ·ä¾†
)

# 2. ã€é—œéµã€‘åªåˆ‡å‡ºå‰ 16 å¼µåœ–ç‰‡
# ä½¿ç”¨ torch.utils.data.Subset
indices = range(16) # å–ç¬¬ 0 åˆ°ç¬¬ 15 å¼µ
train_dataset = torch.utils.data.Subset(full_dataset, indices)

print(f"âš ï¸ æ­£åœ¨é€²è¡Œéæ“¬åˆæ¸¬è©¦ï¼")
print(f"âš ï¸ è¨“ç·´è³‡æ–™æ•¸é‡: {len(train_dataset)} (åŸæœ¬æ˜¯ 1000)")

# 3. ã€é—œéµã€‘DataLoader è¨­å®š
# shuffle=False: ä¸è¦äº‚æ•¸æ´—ç‰Œï¼Œé¡Œç›®é †åºå›ºå®šï¼Œè®“æ¨¡å‹æ›´å¥½èƒŒ
# batch_size=16: ä¸€æ¬¡å°±æŠŠé€™ 16 å¼µå…¨çœ‹å®Œ
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)
'''

train_dataset = MyDataset(df, img_path)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# ------------------------------------
# 3. åˆå§‹åŒ–æ¨¡å‹ã€Lossã€å„ªåŒ–å™¨
# ------------------------------------
cnn = CNN().to("cuda")

# loss_func = nn.CrossEntropyLoss(label_smoothing=0.1).to("cuda")
loss_func = nn.CrossEntropyLoss().to("cuda")
optimizer = optim.Adam(cnn.parameters(), lr=Learning_Rate)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode = 'min', factor = 0.1, patience = 3
)


# ------------------------------------
# 4. é–‹å§‹è¨“ç·´
# ------------------------------------
epoch_losses = []
if os.path.exists("pano_cnn_model.pth"):
    cnn.load_state_dict(torch.load('pano_cnn_model.pth'))

cnn.train()

for epoch in range(Num_Epoch):
    running_loss = 0.0

    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    with tqdm(train_loader, desc=f"Epoch {epoch + 1}/{Num_Epoch}", ncols = 100, leave = True) as loop:

        for images, labels, filenames in loop:
            # A. æ¬ç§»è³‡æ–™åˆ° GPU
            images = images.to("cuda")
            labels = labels.to("cuda")

            # B. æ­¸é›¶æ¢¯åº¦
            optimizer.zero_grad()

            # C. Forward Pass
            outputs = cnn(images)

            # D. è¨ˆç®— Loss
            loss = loss_func(outputs, labels)

            # E. Backward Pass
            loss.backward()

            # F. æ›´æ–°åƒæ•¸
            optimizer.step()

            # --- ç´€éŒ„æ•¸æ“š ---
            running_loss += loss.item()

            # é€²åº¦èª¿é¡¯ç¤ºå³æ™‚ loss
            loop.set_postfix(loss=loss.item())


        # å°å‡ºé€™ä¸€å€‹ Epoch çš„å¹³å‡ Loss
        avg_loss = running_loss / len(train_loader)
        epoch_losses.append(avg_loss)
        scheduler.step(avg_loss)
        print(f"Epoch {epoch+1} Result: Loss={avg_loss:.4f}")

# --------------------------
# 5. ç•«å‡º Loss æŠ˜ç·šåœ–
# --------------------------
plt.figure(figsize=(10, 5))
plt.plot(epoch_losses, label='Training Loss')
plt.title('Training Loss Trend')
plt.xlabel('Epochs')
plt.ylabel('Average Loss')
plt.legend()
plt.grid(True)
plt.xticks(range(0, Num_Epoch, 2))

# å­˜æª”è€Œä¸æ˜¯åªæœ‰é¡¯ç¤º (æ–¹ä¾¿åœ¨ Server ä¸Šçœ‹)
plt.savefig('loss_curve.png')
print("è¨“ç·´çµæŸï¼Loss åœ–è¡¨å·²å„²å­˜ç‚º loss_curve.png")

# å„²å­˜æ¨¡å‹æ¬Šé‡
torch.save(cnn.state_dict(), 'pano_cnn_model.pth')
print("æ¨¡å‹å·²å„²å­˜ç‚º pano_cnn_model.pth")